---
layout: post
title: "Testing: The final functional frontier"
description: After making the jump from OOP to FP, how do we go from OOT to FT?
date: 2023-03-21 18:40:45 +0000
---

A lot of languages these days are either strictly functional programming languages or at the very least _enable_ functional programming. However when working in one of these codebases I feel there is often a mental divide I would like to explore.

In our source code (our Dr. Jekyll code if you will) we find mostly pure functions, declarative effects and immutable data. Side-effect interpretation will be a small part of the codebase, often only the entrypoint. Then we open the test directory, suddenly Mr. Hyde rears his ugly head. Side-effects as far as the eye can see, mutable shared class state, exceptions thrown left right and center and referential opacity runs rampant.

In this directory there are no laws, all rules of F.P. seem to vanish into the ether! I think we can look at breaking down this divide. Personally I believe it exists due to a lack of support for carrying these principles easily into our test code.

Before going further, I would like to note that if you practice OOP this post isn't likely for you and that is fine. If however you are an F.P. practitioner, let's start by exploring a simple example test:

```
// Your test cases will likely look like this pseudo code.
// Note that "container" means class/namespace/whatever works in your language.

container ${Suite Name} {

    mutable-variable database = null

    beforeAll() {
        database = setupDatabaseConnection()
    }

    afterEach() {
        clearTables(database)
    }

    afterAll() {
        database.close()
    }

    method ${First Test Name} {
        // First test code using database
        if(!condition) { throw Error("Test error message 1") }
    }

    method ${Second Test Name} {
        // Second test code using database
        if(!condition) { throw Error("Test error message 2") }
    }

}
```

So whats the big deal here? There is nothing _technically wrong_ with the above, the tests will run and we get errors if they fail. I'm going to list just a few issues as I see them with this method of testing with functional programming principals in mind.

1. All functions return `void` which to me is usually a sign that a function is performing side effects/mutation.
   - Our tests may (and likely will) throw an error
   - Our tests may log to console directly themselves
   - We are likely to open close file/database handles midtest
   - Lifecycle methods may also do the above
1. Our database instance is initialised as `null` and is a **mutable variable** on our container.
1. Our container is implementing a lifecycle in a very OO manner, it is aware it needs to be called to setup and teardown.
1. Depending on the framework the lifcycle handling may cause unforseen problems:
   - What happens if our setup function fails? Do we abort or continue?
   - What if either teardown function fails?
1. It is not always obvious how are tests are being invoked, we never call the test functions ourselves, there is some dark sorcery at work here!
   - Are they deterministic?
   - Do we have control over order/parallelism?
1. We often have common fixtures across multiple suites. How do we make use of our database connection in another suite?
1. How do we share functionality or common test scenarios? This style of testing will often lead to inheritence trees over composition.

I want to stress again that this is just my opinion on how we can improve our test code, continuing in this style has worked for many years and I in no way am implying everyone is wrong and needs to change. That being said, I think even this simple exploration into testing highlights some areas of improvement, so, let's look at:

## A different approach

Let's try to approach each of the problems I have mentioned above. Imagine new functions and types such as `Suite` and `Using` have been implemented as part of our new functional framework.

```
constant-value mySuite = function(database =>
    Suite("Suite Name",
        [
            Test("First Test Name", {
                // First test code using database
                if(condition) Pass else Fail("Test error message 1")
            }),
            Test("Second Test Name", {
                // Second test code using database
                if(condition) Pass else Fail("Test error message 2")
            }),
        ]
    ))
```

1\. Our test functions are real functions! They have type `() => Result` and become part of the functional core of our tests.<br />
3\. Our suite is unaware of test lifecycle, it just knows "I need a database, then here are the tests" as a good function should.<br />
5\. Our tests are now data, they do not get invoked by the dark lord!<br />
Bonus Point: It is now easy to see where the tests terminate and what they do, in normal test frameworks you can write an empty test that does nothing or a test that has 4 million assertions.

That's all well and good, so how would we go about running such tests? We still need some way to interpret these effects and some way of getting our database connection.

```
constant-value myFullSuite = Using(setupDatabaseConnection, function(
    database =>
        constant-value suite = mySuite(database)

        constant-value suiteWithTearDown = AfterEach(suite,
            { if(clearTables(database)) Continue else Abort }
        )
        AfterAll(
            suiteWithTearDown,
            { if(!database.close())
                { LogAndContinue("connection could not close") }
            else
              Continue
            }
        )
    )

run(myFullSuite)
```

1\. Our lifecycle functions are now functions! No more `void` and no more `throw`ing.<br />
2\. Our database connection is now immutable and never `null`.<br />
4\. Lifecycle handling is now clearly defined as part of the test framework and separate from the definition of tests.<br />

- If our setup function fails no tests can be run because they _require_ the database.
- If our teardown function fails we have told the framework to `Abort`. Perhaps in some cases a teardown failure would be tolerable? That's your choice explicitly.
- If our final teardown function fails we can see we just log a message then continue.
- Our lifecycle functions also return effect values.<br/>

5\. It is now clear where everything is being invoked from, we can trace the object hierarchy to see what will be included in the `run` function call. We can order these tests however we like and regain our determinism<br/>
6\. Sharing a fixture accross suites is also now trivial since the fixture creation is independant of what tests use it.

### Composability!

My final point in the issue list above was about composition and inheritence. For me this is one of the most important points, due to the more functional nature our tests become composable and reusable. Let's run through a couple of small examples of how this is could be used.

```
constant-value someTests = // TestSuite defined as in example above.

// Repeat a test very easily by mapping to a list of [test, test, test, ...]
constant-value repeated = repeat(someTests, 10)

// Create dependant tests by composing different functions together
whenPassing(someTests, function(() => nextTests()))

// Expect tests to fail by simply flipping the results
invert(someTests)
```

This is obviously a very simple example, but you can see how it could be more generalised. You could use this to implement some kind of automatic contract/law testing suites (for example [Comparable](https://docs.oracle.com/javase/8/docs/api/java/lang/Comparable.html) in java or [Functor laws](https://wiki.haskell.org/Functor#Functor_Laws)). Another example would be parameterised tests, you would not need to build parameterisation into the test framework because you can just `map` over a list of inputs to create the `Test`s yourself. Manipulation of this data would be handled the same way you handle any other data!

## Conclusion

I think we can extend our FP principals to not just our source code, but our test code aswell. This would give us all the same benefits that F.P. brings us in our source code e.g. referential transparancy, improved readability, immutabaility etc. This is just a hunch, but I also believe it would aid in letting clients use test frameworks in unexpected and interesting ways, it's always difficult to customise a test framework that does all the heavy lifting for you, for example running tests in a docker container.

Are there any such libraries in existence already?

- If you use Haskell then [HUnit](http://hackage.haskell.org/package/HUnit) is a good example (unsurprisingly those Haskell Rascals!).
- For Clojure [Clojure Test](https://clojure.github.io/clojure/clojure.test-api.html) and for scala [uTest](https://github.com/lihaoyi/utest) are both frameowrks I feel move in the right direction. Tests tend to be declarative in nature but still have an OO feel and aren't always first class citizens to be operated on.

If you have read this far then thank you ðŸ˜€
